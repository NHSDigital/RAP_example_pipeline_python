{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAP Example Python Pipeline - Interactive Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/NHSDigital/RAP_example_pipeline_python/rap_example_pipeline_python.ipynb\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of the script:  to provide an example of good practices when structuring a pipeline using PySpark\n",
    "\n",
    "The script loads Python packages but also internal modules (e.g. modules.helpers, helpers script from the modules folder).\n",
    "It then loads various configuration variables and a logger, for more info on see the RAP Community of Practice website:\n",
    "https://nhsdigital.github.io/rap-community-of-practice/\n",
    "\n",
    "Most of the code to carry out this configuration and setup is found in the utils folder.\n",
    "\n",
    "Then, the main pipeline itself begins, which has three phases:\n",
    "\n",
    "data_ingestion: \n",
    "    we download the artificial hes data, load it into a spark dataframe. Any other cleaning or preprocessing should\n",
    "    happen at this stage\n",
    "processing: \n",
    "    we process the data as needed, in this case we create some aggregate counts based on the hes data\n",
    "data_exports: \n",
    "    finally we write our outputs to an appropriate file type (CSV)\n",
    "\n",
    "Note that in the src folder, each of these phases has its own folder, to neatly organise the code used for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part imports our Python packages, pyspark functions, and our project's own modules\n",
    "import logging\n",
    "import timeit \n",
    "from datetime import datetime \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from src.utils import file_paths\n",
    "from src.utils import logging_config\n",
    "from src.utils import spark as spark_utils\n",
    "from src.data_ingestion import get_data\n",
    "from src.data_ingestion import reading_data\n",
    "from src.processing import aggregate_counts\n",
    "from src.data_exports import write_csv\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # load config, here we load our project's parameters from the config.toml file\n",
    "    config = file_paths.get_config() \n",
    "\n",
    "    # configure logging\n",
    "    logging_config.configure_logging(config['log_dir'])\n",
    "    logger.info(f\"Configured logging with log folder: {config['log_dir']}.\")\n",
    "    logger.info(f\"Logging the config settings:\\n\\n\\t{config}\\n\")\n",
    "    logger.info(f\"Starting run at:\\t{datetime.now().time()}\")\n",
    "\n",
    "    # get artificial HES data as CSV\n",
    "    get_data.download_zip_from_url(config['data_url'], overwrite=True)\n",
    "    logger.info(f\"Downloaded artificial hes as zip.\")\n",
    "\n",
    "    # create spark session\n",
    "    spark = spark_utils.create_spark_session(config['project_name'])\n",
    "    logger.info(f\"created spark session with app name: {config['project_name']}\")\n",
    "\n",
    "    # Loading data from CSV as spark data frame\n",
    "    df_hes_data = reading_data.load_csv_into_spark_data_frame(spark, config['path_to_downloaded_data'])\n",
    "\n",
    "    # Creating dictionary to hold outputs\n",
    "    outputs = {}\n",
    "\n",
    "    # Count number of episodes in England - place this in the outputs dictionary\n",
    "    outputs[\"df_hes_england_count\"] = aggregate_counts.get_distinct_count(df_hes_data, 'epikey', 'number_of_episodes')\n",
    "\n",
    "    # Rename and save spark dataframes as CSVs:\n",
    "    for output_name, output in outputs.items():\n",
    "        write_csv.save_spark_dataframe_as_csv(output, output_name)\n",
    "        logger.info(f\"saved output df {output_name} as csv\")\n",
    "        write_csv.rename_csv_output(output_name)\n",
    "        logger.info(f\"renamed {output_name} file\")\n",
    "    \n",
    "    # stop the spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f\"Running create_publication script\")\n",
    "    start_time = timeit.default_timer()\n",
    "    main()\n",
    "    total_time = timeit.default_timer() - start_time\n",
    "    logger.info(f\"Running time of create_publication script: {int(total_time / 60)} minutes and {round(total_time%60)} seconds.\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
