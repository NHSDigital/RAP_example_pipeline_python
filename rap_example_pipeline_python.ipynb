{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAP Example Python Pipeline - Interactive Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/NHSDigital/RAP_example_pipeline_python/rap_example_pipeline_python.ipynb\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose of the script:  to provide an example of good practices when structuring a pipeline using PySpark\n",
    "\n",
    "The script loads Python packages but also internal modules (e.g. modules.helpers, helpers script from the modules folder).\n",
    "It then loads various configuration variables and a logger, for more info on see the RAP Community of Practice website:\n",
    "https://nhsdigital.github.io/rap-community-of-practice/\n",
    "\n",
    "Most of the code to carry out this configuration and setup is found in the utils folder.\n",
    "\n",
    "Then, the main pipeline itself begins, which has three phases:\n",
    "\n",
    "data_ingestion: \n",
    "    we download the artificial hes data, load it into a spark dataframe. Any other cleaning or preprocessing should\n",
    "    happen at this stage\n",
    "processing: \n",
    "    we process the data as needed, in this case we create some aggregate counts based on the hes data\n",
    "data_exports: \n",
    "    finally we write our outputs to an appropriate file type (CSV)\n",
    "\n",
    "Note that in the src folder, each of these phases has its own folder, to neatly organise the code used for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part imports our Python packages, pyspark functions, and our project's own modules\n",
    "import logging\n",
    "import timeit \n",
    "from datetime import datetime \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from src.utils import file_paths\n",
    "from src.utils import logging_config\n",
    "from src.utils import spark as spark_utils\n",
    "from src.data_ingestion import get_data\n",
    "from src.data_ingestion import reading_data\n",
    "from src.processing import aggregate_counts\n",
    "from src.data_exports import write_csv\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = file_paths.get_config() \n",
    "\n",
    "# configure logging\n",
    "logging_config.configure_logging(config['log_dir'])\n",
    "logger.info(f\"Configured logging with log folder: {config['log_dir']}.\")\n",
    "logger.info(f\"Logging the config settings:\\n\\n\\t{config}\\n\")\n",
    "logger.info(f\"Starting run at:\\t{datetime.now().time()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get artificial HES data as CSV\n",
    "get_data.download_zip_from_url(config['data_url'], overwrite=True)\n",
    "logger.info(f\"Downloaded artificial hes as zip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_hes_data = pd.read_csv(config['path_to_downloaded_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_count(df: pd.DataFrame, col_to_aggregate: str) -> int:\n",
    "    \"\"\"Returns the number of distinct values in a column of a pandas DataFrame.\"\"\"\n",
    "    return df[col_to_aggregate].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary to hold outputs\n",
    "outputs = {}\n",
    "\n",
    "# Count number of episodes in England - place this in the outputs dictionary\n",
    "outputs[\"df_hes_england_count\"] = get_distinct_count(df_hes_data, 'EPIKEY')\n",
    "\n",
    "# Rename and save spark dataframes as CSVs:\n",
    "for output_name, output in outputs.items():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with the integer value\n",
    "    df_output = pd.DataFrame({'england_count': [outputs[\"df_hes_england_count\"]]})\n",
    "\n",
    "    # prep the filepath and ensure the directory exists\n",
    "    from pathlib import Path\n",
    "    output_file = 'my_file.csv'\n",
    "    output_dir = Path(f'data_out/{output_name}')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_filename = output_dir /f'{output_name}.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_output.to_csv(output_filename, index=False)\n",
    "    logger.info(f\"saved output df to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
