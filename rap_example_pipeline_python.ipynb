{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAP Example Python Pipeline - Interactive Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD THE GOOGLE COLAB LINK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This notebook will show you how straight-forward it is to do an analytical pipeline in Python.\n",
    "\n",
    "The core of any of piece of analytical work is to:\n",
    "- load some data\n",
    "- do something to do that, e.g. process it, do some analysis\n",
    "- create some output\n",
    "\n",
    "This notebook will go briefly through each of these showing *one* way of doing it in Python (there are many more!). \n",
    "\n",
    "Open this notebook in google colab and have a play - try changing bits and see what happens!\n",
    "\n",
    "**NOTE**: to make the workshop more straight forward, we haven't completely followed good practice. If you want to see a pipeline how it should be, well laid out and modularised, [see our Example Python pipeline](https://github.com/NHSDigital/RAP_example_pipeline_python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install a few things before we can get going.\n",
    "\n",
    "First, if this is running in Google Colab, we need to clone the repo and install the right python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this forces google collab to install the dependencies\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on Colab\")\n",
    "    !git clone https://github.com/NHSDigital/RAP_example_pipeline_python.git -q\n",
    "    %cd RAP_example_pipeline_python\n",
    "    !pip install -r requirements.txt -q -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to import the right libraries for this piece of work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # this allows us to write log messages helping with any future auditing and debugging\n",
    "import timeit # this allows us to time the execution of the code\n",
    "from datetime import datetime # this allows us to work with dates and times\n",
    "import pandas as pd # this allows us to work with dataframes\n",
    "\n",
    "# these are the modules we have created to help us with the pipeline\n",
    "from src.utils import file_paths\n",
    "from src.utils import logging_config\n",
    "from src.utils import spark as spark_utils\n",
    "import src.data_ingestion\n",
    "from src.processing import aggregate_counts\n",
    "from src.data_exports import write_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that we don't hardcode things which can change into the code - instead we keep things like that in config files.\n",
    "\n",
    "An example is where the data is to be picked up from and where any outputs will be saved to: these will change from when you are working in \"dev\" to when the code is finalised and put into \"production\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = file_paths.get_config() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise and configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging_config.configure_logging(config['log_dir'])\n",
    "logger.info(f\"Configured logging with log folder: {config['log_dir']}.\")\n",
    "logger.info(f\"Logging the config settings:\\n\\n\\t{config}\\n\")\n",
    "logger.info(f\"Starting run at:\\t{datetime.now().time()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data: we're going to use an artificial fake version of the NHS Hospital Episode Statistics Accident and Emergency (HES AE) data from 2003. \n",
    "\n",
    "We've hidden all the complexity of aquiring the data away in a function - called \"get_data\". This is good practice, because:\n",
    "\n",
    "1. this data might be used many times in many different pipelines - this function can be reused, saving your colleagues time\n",
    "2. the way the data is acquired might change, e.g. in different platforms, to accomodate this we only need to add to, change or improve this function - your downstream pipeline should continue as normal\n",
    "\n",
    "This function:\n",
    "- gets the location of the data from the config file\n",
    "- downloads the CSV\n",
    "- loads that CSV into a pandas dataframe in memory\n",
    "\n",
    "This is just an example - in another setting we could make it load the data from a SQL server, or from a database, S3 bucket, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    \"\"\"Get the data from the data source and return it as a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "        config (dict): the configuration dictionary\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: the data    \n",
    "    \"\"\"\n",
    "\n",
    "    # get the data location from the config\n",
    "    data_location = config['data_url']\n",
    "    print(\"the data came from here: \", data_location) # let's print the location so you can see where it is stored - it's a publicly available zip.\n",
    "\n",
    "    # download the CSV file\n",
    "    src.data_ingestion.get_data.download_zip_from_url(data_location, overwrite=True)\n",
    "    logger.info(f\"Downloaded data as zip.\")\n",
    "\n",
    "    # read the CSV file into a pandas dataframe\n",
    "    df_data = pd.read_csv(config['path_to_downloaded_data'])\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use our get data function to... get the data! Look how simple it makes the code below to read - it does what it says on the tin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes_data = get_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this data looks like, and pull the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hes_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun part - we get to do some interesting processing on the data.\n",
    "\n",
    "The simplest piece of processing you might do is simply get a distinct count on one of the columns. \n",
    "\n",
    "Again, we create a function to do this - for a very small bit of processing like this it might not make a lot of sense, but if you were doing a larger derivation that might feasibly be used in other work, it could really save someone else some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_count(df: pd.DataFrame, col_to_aggregate: str) -> int:\n",
    "    \"\"\"Returns the number of distinct values in a column of a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the pandas DataFrame\n",
    "        col_to_aggregate (str): the column to aggregate\n",
    "\n",
    "    Returns:\n",
    "        int: the number of distinct values\n",
    "    \"\"\"\n",
    "    return df[col_to_aggregate].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our simple analysis and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_epikey_count = get_distinct_count(df_hes_data, 'EPIKEY')\n",
    "print(f\"Distinct EPIKEY count: {distinct_epikey_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary to hold outputs\n",
    "outputs = {}\n",
    "\n",
    "# Count number of episodes in England - place this in the outputs dictionary\n",
    "outputs[\"df_hes_england_count\"] = get_distinct_count(df_hes_data, 'EPIKEY')\n",
    "\n",
    "# Rename and save spark dataframes as CSVs:\n",
    "for output_name, output in outputs.items():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with the integer value\n",
    "    df_output = pd.DataFrame({'england_count': [outputs[\"df_hes_england_count\"]]})\n",
    "\n",
    "    # prep the filepath and ensure the directory exists\n",
    "    from pathlib import Path\n",
    "    output_file = 'my_file.csv'\n",
    "    output_dir = Path(f'data_out/{output_name}')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_filename = output_dir /f'{output_name}.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_output.to_csv(output_filename, index=False)\n",
    "    logger.info(f\"saved output df to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
