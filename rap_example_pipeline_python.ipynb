{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAP Example Python Pipeline - Interactive Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD THE GOOGLE COLAB LINK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This notebook will show you how straight-forward it is to do an analytical pipeline in Python.\n",
    "\n",
    "The core of any of piece of analytical work is to:\n",
    "- load some data\n",
    "- do something to do that, e.g. process it, do some analysis\n",
    "- create some output\n",
    "\n",
    "This notebook will go briefly through each of these showing *one* way of doing it in Python (there are many more!). \n",
    "\n",
    "Open this notebook in google colab and have a play - try changing bits and see what happens!\n",
    "\n",
    "**NOTE**: to make the workshop more straight forward, we haven't completely followed good practice. If you want to see a pipeline how it should be, well laid out and modularised, [see our Example Python pipeline](https://github.com/NHSDigital/RAP_example_pipeline_python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install a few things before we can get going.\n",
    "\n",
    "First, if this is running in Google Colab, we need to clone the repo and install the right python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this forces google collab to install the dependencies\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    print(\"Running on Colab\")\n",
    "    !git clone https://github.com/NHSDigital/RAP_example_pipeline_python.git -q\n",
    "    %cd RAP_example_pipeline_python\n",
    "    !pip install -r requirements.txt -q -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to import the right libraries for this piece of work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # this allows us to write log messages helping with any future auditing and debugging\n",
    "import timeit # this allows us to time the execution of the code\n",
    "from datetime import datetime # this allows us to work with dates and times\n",
    "import pandas as pd # this allows us to work with dataframes\n",
    "\n",
    "# these are the modules we have created to help us with the pipeline\n",
    "from src.utils import file_paths\n",
    "from src.utils import logging_config\n",
    "from src.utils import spark as spark_utils\n",
    "import src.data_ingestion\n",
    "from src.processing import aggregate_counts\n",
    "from src.data_exports import write_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that we don't hardcode things which can change into the code - instead we keep things like that in config files.\n",
    "\n",
    "An example is where the data is to be picked up from and where any outputs will be saved to: these will change from when you are working in \"dev\" to when the code is finalised and put into \"production\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = file_paths.get_config() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-10 11:42:34,860 - INFO -- 1487816006.py:                <module>():4 -- Configured logging with log folder: .\n",
      "2024-06-10 11:42:34,861 - INFO -- 1487816006.py:                <module>():5 -- Logging the config settings:\n",
      "\n",
      "\t{'project_name': 'example_pipeline_pyspark_version', 'data_url': 'https://files.digital.nhs.uk/assets/Services/Artificial%20data/Artificial%20HES%20final/artificial_hes_ae_202302_v1_sample.zip', 'path_to_downloaded_data': 'data_in/artificial_hes_ae_202302_v1_sample.zip/artificial_hes_ae_202302_v1_sample/artificial_hes_ae_2122.csv', 'output_dir': '', 'log_dir': ''}\n",
      "\n",
      "2024-06-10 11:42:34,862 - INFO -- 1487816006.py:                <module>():6 -- Starting run at:\t11:42:34.862941\n"
     ]
    }
   ],
   "source": [
    "# initialise and configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging_config.configure_logging(config['log_dir'])\n",
    "logger.info(f\"Configured logging with log folder: {config['log_dir']}.\")\n",
    "logger.info(f\"Logging the config settings:\\n\\n\\t{config}\\n\")\n",
    "logger.info(f\"Starting run at:\\t{datetime.now().time()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data: we're going to use an artificial fake version of the NHS Hospital Episode Statistics Accident and Emergency (HES AE) data from 2003. \n",
    "\n",
    "We've hidden all the complexity of aquiring the data away in a function - called \"get_data\". This is good practice, because:\n",
    "\n",
    "1. this data might be used many times in many different pipelines - this function can be reused, saving your colleagues time\n",
    "2. the way the data is acquired might change, e.g. in different platforms, to accomodate this we only need to add to, change or improve this function - your downstream pipeline should continue as normal\n",
    "\n",
    "This function:\n",
    "- gets the location of the data from the config file\n",
    "- downloads the CSV\n",
    "- loads that CSV into a pandas dataframe in memory\n",
    "\n",
    "This is just an example - in another setting we could make it load the data from a SQL server, or from a database, S3 bucket, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    \"\"\"Get the data from the data source and return it as a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "        config (dict): the configuration dictionary\n",
    "\n",
    "    Returns:\n",
    "        pandas dataframe: the data    \n",
    "    \"\"\"\n",
    "\n",
    "    # get the data location from the config\n",
    "    data_location = config['data_url']\n",
    "    print(\"the data came from here: \", data_location) # let's print the location so you can see where it is stored - it's a publicly available zip.\n",
    "\n",
    "    # download the CSV file\n",
    "    src.data_ingestion.get_data.download_zip_from_url(data_location, overwrite=True)\n",
    "    logger.info(f\"Downloaded data as zip.\")\n",
    "\n",
    "    # read the CSV file into a pandas dataframe\n",
    "    df_data = pd.read_csv(config['path_to_downloaded_data'])\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use our get data function to... get the data! Look how simple it makes the code below to read - it does what it says on the tin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data came from here:  https://files.digital.nhs.uk/assets/Services/Artificial%20data/Artificial%20HES%20final/artificial_hes_ae_202302_v1_sample.zip\n",
      "2024-06-10 11:42:38,509 - INFO -- 734666532.py:                get_data():10 -- Downloaded data as zip.\n"
     ]
    }
   ],
   "source": [
    "df_hes_data = get_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this data looks like, and pull the first 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FYEAR</th>\n",
       "      <th>PARTYEAR</th>\n",
       "      <th>PSEUDO_HESID</th>\n",
       "      <th>AEKEY</th>\n",
       "      <th>AEKEY_FLAG</th>\n",
       "      <th>AEARRIVALMODE</th>\n",
       "      <th>AEATTEND_EXC_PLANNED</th>\n",
       "      <th>AEATTENDCAT</th>\n",
       "      <th>AEATTENDDISP</th>\n",
       "      <th>AEDEPTTYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>LSOA11</th>\n",
       "      <th>MSOA11</th>\n",
       "      <th>PROVDIST</th>\n",
       "      <th>PROVDIST_FLAG</th>\n",
       "      <th>NER_GP_PRACTICE</th>\n",
       "      <th>NER_RESIDENCE</th>\n",
       "      <th>NER_TREATMENT</th>\n",
       "      <th>SITETRET</th>\n",
       "      <th>SITEDIST</th>\n",
       "      <th>SITEDIST_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2122</td>\n",
       "      <td>202103</td>\n",
       "      <td>TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK</td>\n",
       "      <td>910587081231</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>E01000385</td>\n",
       "      <td>E02001768</td>\n",
       "      <td>19.37</td>\n",
       "      <td>3.0</td>\n",
       "      <td>QKS</td>\n",
       "      <td>QKS</td>\n",
       "      <td>QHM</td>\n",
       "      <td>RW601</td>\n",
       "      <td>4.89</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2122</td>\n",
       "      <td>202103</td>\n",
       "      <td>TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK</td>\n",
       "      <td>747777461989</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>E01030571</td>\n",
       "      <td>E02004833</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.0</td>\n",
       "      <td>QMJ</td>\n",
       "      <td>QYG</td>\n",
       "      <td>QKS</td>\n",
       "      <td>RY901</td>\n",
       "      <td>1.21</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2122</td>\n",
       "      <td>202103</td>\n",
       "      <td>TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK</td>\n",
       "      <td>244053969711</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>E01008938</td>\n",
       "      <td>E02005828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>QWE</td>\n",
       "      <td>QKK</td>\n",
       "      <td>QWO</td>\n",
       "      <td>RJC02</td>\n",
       "      <td>15.16</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2122</td>\n",
       "      <td>202103</td>\n",
       "      <td>TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK</td>\n",
       "      <td>425257514835</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>E01030533</td>\n",
       "      <td>E02000912</td>\n",
       "      <td>23.68</td>\n",
       "      <td>3.0</td>\n",
       "      <td>QMJ</td>\n",
       "      <td>QRV</td>\n",
       "      <td>QOP</td>\n",
       "      <td>RJE07</td>\n",
       "      <td>3.16</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2122</td>\n",
       "      <td>202103</td>\n",
       "      <td>TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK</td>\n",
       "      <td>892001219292</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>E01025434</td>\n",
       "      <td>E02004315</td>\n",
       "      <td>5.85</td>\n",
       "      <td>3.0</td>\n",
       "      <td>QMF</td>\n",
       "      <td>QM7</td>\n",
       "      <td>QMJ</td>\n",
       "      <td>RDE03</td>\n",
       "      <td>2.41</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FYEAR  PARTYEAR                      PSEUDO_HESID         AEKEY  \\\n",
       "0   2122    202103  TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK  910587081231   \n",
       "1   2122    202103  TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK  747777461989   \n",
       "2   2122    202103  TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK  244053969711   \n",
       "3   2122    202103  TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK  425257514835   \n",
       "4   2122    202103  TESTqPNh7HEHdm1sB5QlvVaSQZS7BekK  892001219292   \n",
       "\n",
       "   AEKEY_FLAG  AEARRIVALMODE  AEATTEND_EXC_PLANNED  AEATTENDCAT  AEATTENDDISP  \\\n",
       "0           1              2                     1            1             3   \n",
       "1           1              2                     1            1             3   \n",
       "2           1              2                     1            1             3   \n",
       "3           1              2                     1            1             1   \n",
       "4           1              2                     1            1             3   \n",
       "\n",
       "   AEDEPTTYPE  ...     LSOA11     MSOA11  PROVDIST PROVDIST_FLAG  \\\n",
       "0           1  ...  E01000385  E02001768     19.37           3.0   \n",
       "1           1  ...  E01030571  E02004833      3.96           3.0   \n",
       "2           3  ...  E01008938  E02005828       NaN           3.0   \n",
       "3           1  ...  E01030533  E02000912     23.68           3.0   \n",
       "4           1  ...  E01025434  E02004315      5.85           3.0   \n",
       "\n",
       "  NER_GP_PRACTICE NER_RESIDENCE  NER_TREATMENT  SITETRET SITEDIST  \\\n",
       "0             QKS           QKS            QHM     RW601     4.89   \n",
       "1             QMJ           QYG            QKS     RY901     1.21   \n",
       "2             QWE           QKK            QWO     RJC02    15.16   \n",
       "3             QMJ           QRV            QOP     RJE07     3.16   \n",
       "4             QMF           QM7            QMJ     RDE03     2.41   \n",
       "\n",
       "   SITEDIST_FLAG  \n",
       "0            5.0  \n",
       "1            5.0  \n",
       "2            5.0  \n",
       "3            5.0  \n",
       "4            5.0  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hes_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun part - we get to do some interesting processing on the data.\n",
    "\n",
    "The simplest piece of processing you might do is simply get a distinct count on one of the columns. \n",
    "\n",
    "Again, we create a function to do this - for a very small bit of processing like this it might not make a lot of sense, but if you were doing a larger derivation that might feasibly be used in other work, it could really save someone else some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_count(df: pd.DataFrame, col_to_aggregate: str) -> int:\n",
    "    \"\"\"Returns the number of distinct values in a column of a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the pandas DataFrame\n",
    "        col_to_aggregate (str): the column to aggregate\n",
    "\n",
    "    Returns:\n",
    "        int: the number of distinct values\n",
    "    \"\"\"\n",
    "    return df[col_to_aggregate].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our simple analysis and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct EPIKEY count: 10000\n"
     ]
    }
   ],
   "source": [
    "distinct_epikey_count = get_distinct_count(df_hes_data, 'EPIKEY')\n",
    "print(f\"Distinct EPIKEY count: {distinct_epikey_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-28 14:18:00,084 - INFO -- 1024386828.py:                <module>():24 -- saved output df to data_out/df_hes_england_count/df_hes_england_count.csv\n"
     ]
    }
   ],
   "source": [
    "# Creating dictionary to hold outputs\n",
    "outputs = {}\n",
    "\n",
    "# Count number of episodes in England - place this in the outputs dictionary\n",
    "outputs[\"df_hes_england_count\"] = get_distinct_count(df_hes_data, 'EPIKEY')\n",
    "\n",
    "# Rename and save spark dataframes as CSVs:\n",
    "for output_name, output in outputs.items():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with the integer value\n",
    "    df_output = pd.DataFrame({'england_count': [outputs[\"df_hes_england_count\"]]})\n",
    "\n",
    "    # prep the filepath and ensure the directory exists\n",
    "    from pathlib import Path\n",
    "    output_file = 'my_file.csv'\n",
    "    output_dir = Path(f'data_out/{output_name}')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_filename = output_dir /f'{output_name}.csv'\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_output.to_csv(output_filename, index=False)\n",
    "    logger.info(f\"saved output df to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
